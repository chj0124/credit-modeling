#+TITLE: 待添加内容
#+TAGS: new_file(f) add(a) reference(r) code(c) script(s)

* DONE 只要懂得编程，你就能用这个技能去学习其它的内容                :script:
CLOSED: [2019-04-16 周二 17:52]
* DONE 井外的世界                                                  :new_file:
CLOSED: [2019-04-16 周二 17:55]
** DONE 什么是数学模型
CLOSED: [2019-04-03 周三 17:20]
模型就是变量之间的关系
** TODO 不同行业的数据分析
* TODO 玄学问题.org                                                :new_file:
** DONE 过拟合
CLOSED: [2019-04-16 周二 17:52]
凡是在有限数据集上追求最小误差就是过拟合
** TODO 硬阈值模型与软阈值模型
** DONE 极小值
CLOSED: [2019-04-02 周二 18:04]
这个内容归纳到什么地方？

至少对线性模型来说，从损失函数这个凹函数就可以看出来，根本不存在什么局部最小值，可以放心地使用梯度下降法，唯一需要担心的是不同的学习率带来的效率和精度的问题。

对于复杂的模型，可以通过概率方式证明，那些各个一阶导数全为 0 的点，二阶导数全大于 0 的概率是极低的，也就是说在误差曲面上鞍点很多，局部最小值极少。
* TODO 特征工程.org                                                :new_file:
** woe 与 one-hot
** vif
** 产生衍生变量的几种方法
- 经验法
- GBDT + LR
- 卷积层
- 主成分降维
- MLP
** 判别模型中的单变量陷阱
例子：两个布尔变量的联合概率分布
* TODO sigmoid 带来了多强的非线性表达能力                               :add:
* TODO 逐步回归                                                         :add:
* TODO 即使完美的模型也无法完美地解决问题                               :add:
* TODO 总结五种损失函数                                                 :add:
- [[https://www.jiqizhixin.com/articles/2018-06-21-3][5个回归损失函数]]
- [[https://www.jiqizhixin.com/articles/091202][几种常用的损失函数]]

* TODO 经验风险与结构风险                                               :add:
机器学习中的目标函数、损失函数、代价函数有什么区别？ - zzanswer的回答 - 知乎
https://www.zhihu.com/question/52398145/answer/209358209
* TODO 梯度下降的变体                                                   :add:
- 批量梯度下降
- 随机梯度下降 :: 感知机使用的就是这种方法，其实并没有加入随机元素
- 动量梯度下降
- mini-batch 梯度下降
* TODO 分类与回归的本质区别                                             :add:
* TODO 随机误差项与回归的假设                                           :add:
** Linearity 线性
应变量和每个自变量都是线性关系。
** Indpendence 独立性
对于所有的观测值，它们的误差项相互之间是独立的。
** Normality 正态性
误差项 (y - t) 服从正态分布。(中心极限定理)
** Equal-variance 等方差
所有的误差项具有同样方差。

这四个假设的首字母，合起来就是 LINE
* TODO 对线性回归效果的评价                                             :add:
- R2
* TODO 二项分布的参数估计                                               :add:
* TODO LMS 算法                                                         :add:
* TODO 最小二乘的时间复杂度                                             :add:
o(n^3)

若特征数大于 10，最小二乘在速度上就不如梯度下降
* TODO 后验估计                                                         :add:
* TODO 核函数                                                           :add:
[[https://www.zhihu.com/question/30371867/answer/624493106][知乎]]
* TODO woe 与信息熵                                                     :add:
* TODO 粗分箱与细分箱                                                   :add:
* TODO 增加偏置值                                                      :code:
* TODO 给所有 SGDClassifier 的子类写一个统一的 train() 类方法          :code:
* TODO 损失函数能量够小的时候终止训练                                  :code:
* TODO Scott Menard's book                                        :reference:
