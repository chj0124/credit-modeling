#+TITLE: 建模培训待添加内容

* TODO Add: 什么是数学模型
* TODO Add: sigmoid 的非线性有多强
* TODO Add: 逐步回归
* TODO ADD: 即使完美的模型也无法完美地解决问题
* TODO ADD: 总结五种损失函数
- [[https://www.jiqizhixin.com/articles/2018-06-21-3][5个回归损失函数]]
- [[https://www.jiqizhixin.com/articles/091202][几种常用的损失函数]]

* TODO ADD: 经验风险与结构风险
机器学习中的目标函数、损失函数、代价函数有什么区别？ - zzanswer的回答 - 知乎
https://www.zhihu.com/question/52398145/answer/209358209
* TODO 代码：给所有 SGDClassifier 的子类写一个统一的 train() 类方法
* TODO 代码：损失函数能量够小的时候终止训练
* TODO 代码：增加偏置值
* TODO 极小值
这个内容归纳到什么地方？

至少对线性模型来说，从损失函数这个凹函数就可以看出来，根本不存在什么局部最小值，可以放心地使用梯度下降法，唯一需要担心的是不同的学习率带来的效率和精度的问题。

对于复杂的模型，可以通过概率方式证明，那些各个一阶导数全为 0 的点，二阶导数全大于 0 的概率是极低的，也就是说在误差曲面上鞍点很多，局部最小值极少。
* TODO 梯度下降的变体
- 批量梯度下降
- 随机梯度下降 :: 感知机使用的就是这种方法，其实并没有加入随机元素
- 动量梯度下降
- mini-batch 梯度下降
