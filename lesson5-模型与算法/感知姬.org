#+TITLE: 线性模型

* 分类器
** 输出离散值就是分类？输出连续值就是回归？
** 分类与回归
* 学习感知机这个简陋模型的意义
** 初识分类器
** Hebbian 原则
*** SGD
*** 不均衡样本
* 一个简单数据集
| x1    | x2     | t |
|-------+--------+---|
| 0.3   | 0.7    | 1 |
| - 0.6 | 0.3    | 0 |
| - 0.1 | - 0.8  | 0 |
| 0.1   | - 0.45 | 1 |
* 感知机结构
** 输入与偏置值
** 权值向量
** 激活函数
** 误差反馈
* 训练过程
** 线性组合
权值向量与输入样本做个简单的点积，得到结果 u
** 激活函数
u 进入激活函数 f，得到感知机输出 y = f(u)
** 计算误差
计算感知机输出 y 与真实值 t 的误差:

Error = E = t - y

** 权值往误差减小的方向移动
~为什么误差定义为 t - y 而不是 y - t 呢？~

* 推广：梯度下降求解线性回归
** 线性回归的形式
当权值向量为 w 时，一个线性回归模型的输出可以表示为：

\begin{huge}
\[
f_w(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
\]
\end{huge}

在给特征集 x 加入一个常数列之后，上式可以表示为：

\begin{huge}
\[
f_w(x) = w^Tx
\]
\end{huge}

只从这个调用形式上看，一个线性回归模型相当于把感知机的激活函数从单位阶跃函数换成了 y=x 这个函数。

** 定义误差：损失函数
针对某个特征集合 x，一个线性回归模型的输出为向量 y，y 中的每一个值和 x 中的每一个样本一一对应。而特征集合中的每一个样本同时也与一个目标向量 t 一一对应，那么就可以针对 t 和 y 来定义当 w 确定时这个模型的误差，比如用 MSE(均方误差) 来表示：

\begin{huge}
\[
J_w = \frac{1}{2n}\sum_{i=1}^{n}(y_i - t_i)^2
\]
\end{huge}

其中，$$ \frac{1}{n} $$ 只是用来归一化求和结果的，$$\frac{1}{2}$$ 是为了求导方便。

用来反映模型的误差的函数就被称为损失函数(或代价函数)，线性回归本质上是一个优化问题，优化的目标就是这个损失函数。

** 梯度下降
函数

\begin{huge}
\[
f(x, y) = x^2 + y^2
\]
\end{huge}

的梯度为：

\begin{huge}
\[
\nabla{f} = (\frac{\partial{f}}{\partial{x}}, \frac{\partial{f}}{\partial{y}}) = (2x, 2y)
\]
\end{huge}

该函数的图像：

#+begin_src python :results file
from matplotlib import pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = Axes3D(fig)
X = np.arange(-4, 4, 0.25)
Y = np.arange(-4, 4, 0.25)
X, Y = np.meshgrid(X, Y)
Z = np.sqrt(X**2 + Y**2)

ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')
plt.savefig(r"./bowl.png")
return(r"./bowl.png")
#+end_src

#+RESULTS:
[[file:./bowl.png]]

这个函数是一个凹函数，在求出它的梯度表达式之后，如果在曲面上投点并记为 P，就可以让 P 按照梯度的反方向修改自己的坐标，直到 P 到达曲面的最低点。

由于线性回归的所有信息都存储在权值向量与偏置值中，所以损失函数实际上可以表示成：

\begin{huge}
\[J = f(w, b)\] 
\end{huge}

或

\begin{huge}
\[J = f(w)\] 
\end{huge}

从 MSE 的表达式可以看出，这个损失函数的图像(即误差曲面)其实也是一个凹函数，并且它也连续可导、导数形式简单，因此权值和偏置值每一次训练的时候按照负梯度方向来更新可以使总误差沿着减小最快的方向减小，直到达到极小值。

** MSE 的梯度下降
既然损失函数可以用这样的方式来求极小值，只要按照梯度的反方向来更新权值，就能得到一个损失函数最小、也就是误差最小的线性回归模型。

也就是说，在使用梯度下降方法训练一个线性回归模型时，每次训练时权值的更新按照以下公式进行：

\begin{huge}
\[
w_i(new) = w_i(old) - \alpha\frac{\partial}{\partial{w_i}}J_w
\] 
\end{huge}

\begin{huge}
\[
\alpha : leaning-rate
\] 
\end{huge}

这里姑且先不考虑偏置值的问题(无关紧要)，那么损失函数对权值求导的结果就是梯度，如果损失函数是 MSE，其结果为：

\begin{huge}
\[
\begin{split}
\frac{\partial}{\partial{w_i}}J_w &= \frac{\partial}{\partial{w_i}} \frac{1}{2n} \sum_{i=0}^n (y_i-t_i)^2 \\
&= 2 \cdot \frac{1}{2n} \sum_{i=0}^n (y_i-t_i) \frac{\partial}{\partial{w_i}} \sum_{i=0}^n (y_i-t_i) \\
&= \frac{1}{n} \sum_{i=0}^n (y_i-t_i) \cdot \frac{\partial}{\partial{w_i}}(\sum_{i=0}^n w_ix_i - t) \\
&= \frac{1}{n} \sum_{i=0}^n (y_i-t_i) x_i
\end{split}
\] 
\end{huge}

所以权值的更新公式可以写成：

\begin{huge}
\[
w_i(new) = w_i(old) - \frac{1}{n} \cdot \alpha \sum_{i=0}^n (y_i-t_i) x_i
\] 
\end{huge}
