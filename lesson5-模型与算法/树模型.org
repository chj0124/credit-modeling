#+TITLE: 决策树及其衍生模型

* 分类树与回归树
[[./tree.png]]
* 分裂原则
** 信息熵

*** 信息论中的信息熵

#+NAME: 信息熵在消息论中的定义
#+begin_quote
在信息论中，熵是接收的每条消息中包含的信息的平均量，又被稱為信息熵、信源熵、平均自信息量。这里，消息代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因（下面会有解释），把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。
#+end_quote

[[https://zhuanlan.zhihu.com/p/26486223]]

*** 应用在决策树中的信息熵

信息熵表示的是不确定度。响应比例在特征的各个类别上均匀分布时，不确定度最大，此时熵就最大。当选择某个特征对数据集进行分类时，分类后的数据集信息熵应当做到比分类前的小，其差值称为信息增益。信息增益可以衡量某个特征对分类结果的影响大小。

对某一数据集 D 的不确定性的描述为：

\begin{huge}
\[
H(D) = E[- ln P(x)] = - \sum_{i=1}^n p_i \codt ln(p_i)
\] 
\end{huge}

这个公式就是数据集 D 的信息熵，其中，n 代表目标变量的类别数，pi 代表第 i 个类别的数量占总数的比例。

假设某个数据集 D 有两个类别，其中一个类别的占比是 100%，那么这个数据集是没有什么不确定度的；根据上面的公式，该数据集的信息熵为：

\begin{huge}
\[
H(D) = - (0 \cdot ln(0) + 1 \cdot ln(1)) = 0
\] 
\end{huge}

在只有两个类别的时候，我们可以绘制其中一个类别的数占比 p 与整个数据集的信息熵的关系：

#+BEGIN_SRC ipython :ipyfile ./obipy/p_H.png :results raw drawer
import numpy as np

p1 = np.linspace(0.01, 0.99, 100)
p2 = 1 - p1
H = - (p1 * np.log(p1) + p2 * np.log(p2))

plt.plot(p1, H)
plt.show()
#+END_SRC

#+RESULTS:
:results:
# Out[31]:
[[file:./obipy/p_H.png]]
:end:

如果数据集 D 中两个类别的占比都是 50%，此时整个数据集的不确定性最大，信息熵约为 0.69，

为了减小数据集的不确定性，可以通过某个分类型的特征来对数据集进行划分，比如某一个特征为 "性别"，那么可以根据性别把整个数据集划分为两个数据集，并且可以对这两个数据集计算它们的信息熵；假如说这次划分刚好把目标变量的两个类别完全分开了，那么两个数据集的信息熵都是 0，此时的信息熵减小得最多，也就是信息增益最大，更一般地，信息增益一般用下面的公式来表达：



** gini 不纯度

CART 算法中使用的是 gini 不纯度。gini 不纯度和信息熵、信息增益这些评价指标没有本质区别，但是在决策树离散化某个连续变量的时候，如果不停地计算信息熵，由于涉及到对数计算，计算机的效率会非常很低下，为了提高效率，可以使用表达式更为简单的 gini 不纯度，gini 不纯度实际上是对信息熵的近似。

\begin{huge}
\[
w^Tx = p
\] 
\end{huge}

** 贪心
训练一棵最优的决策树是一个完全 NP 问题。因此, 实际应用时决策树的训练采用贪心算法来达到局部最优。这样的算法没办法得到最优的决策树。

** 损失函数
决策树的损失函数通常是正则化的极大似然函数

* 步骤
** 选择最好的属性
** 选择分裂点
** 剪枝
* 结果
** 分类结果
** 回归结果
* 几种算法
** ID3
** C4.5

比起 ID3，C4.5 通过使用信息增益率来避免过拟合。

C4.5中，增加的熵要除以分割太细的代价，这个比值叫做信息增益率，显然分割太细分母增加，信息增益率会降低。

** C5.0
** CART (Classification And Regression Tree)
- 使用 gini 不纯度
- 剪枝
* 手写决策树的前置知识
** 递归函数
** 树状数据结构的存储和遍历
