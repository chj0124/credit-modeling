#+TITLE: 线性模型的冰山一角
#+startup: latexpreview

* 自适应的感知机(perceptron)
** 初识分类器
** 感知机结构
*** 输入与偏置值
*** 权值向量
*** 激活函数
*** 误差反馈
** Hebbian 原则
*** 权值更新
*** 不均衡样本
** 一个简单数据集
|-------+--------+---|
| x1    | x2     | t |
|-------+--------+---|
| 0.3   | 0.7    | 1 |
| - 0.6 | 0.3    | 0 |
| - 0.1 | - 0.8  | 0 |
| 0.1   | - 0.45 | 1 |
|-------+--------+---|
** 感知机的训练过程
*** 线性组合
权值向量与输入样本做个简单的点积，得到结果 u
*** 激活函数
u 进入激活函数 f，得到感知机输出 y = f(u)
*** 计算误差
计算感知机输出 y 与真实值 t 的误差:

Error = E = t - y

*** 权值往误差减小的方向移动
~为什么误差定义为 t - y 而不是 y - t 呢？~

* 线性回归的假设
* 梯度下降求解线性回归(linear regression)
** 线性回归的形式
当权值向量为 w 时，一个线性回归模型的输出可以表示为：

\begin{huge}
\[
f_w(x) = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
\]
\end{huge}

在给特征集 x 加入一个常数特征之后，上式可以表示为：

\begin{huge}
\[
f_w(x) = w^Tx
\]
\end{huge}

只从模型结构上看，一个线性回归模型相当于把感知机的激活函数从单位阶跃函数换成了 y=x 这个函数。

** 定义误差：损失函数
针对某个特征集合 x，一个线性回归模型的输出为向量 y，y 中的每一个值和 x 中的每一个样本一一对应。而特征集合中的每一个样本同时也与一个目标向量 t 一一对应，那么就可以针对 t 和 y 来定义当 w 确定时这个模型的误差，比如用 MSE(均方误差) 来表示：

\begin{huge}
\[
J_w = \frac{1}{2n}\sum_{i=1}^{n}(y_i - t_i)^2
\]
\end{huge}

其中，$$\frac{1}{2}$$ 是为了求导方便，$$ \frac{1}{n} $$ 只是用来归一化求和结果的，在后面的计算过程中，$$ \frac{1}{n} $$ 会使结果变得过小，所以根据实际情况也可以省略。

用来反映模型的误差的函数就被称为损失函数(或代价函数)，线性回归本质上是一个优化问题，优化的目标就是这个损失函数。

** 梯度下降
函数

\begin{huge}
\[
f(x, y) = x^2 + y^2
\]
\end{huge}

的梯度为：

\begin{huge}
\[
\nabla{f} = (\frac{\partial{f}}{\partial{x}}, \frac{\partial{f}}{\partial{y}}) = (2x, 2y)
\]
\end{huge}

该函数的图像：

#+begin_src python :results file
from matplotlib import pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = Axes3D(fig)
X = np.arange(-4, 4, 0.25)
Y = np.arange(-4, 4, 0.25)
X, Y = np.meshgrid(X, Y)
Z = np.sqrt(X**2 + Y**2)

ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')
plt.savefig(r"./bowl.png")
return(r"./bowl.png")
#+end_src

#+RESULTS:
[[file:./bowl.png]]

这个函数是一个凹函数，在求出它的梯度表达式之后，如果在曲面上投点并记为 P，就可以让 P 按照梯度的反方向修改自己的坐标，直到 P 到达曲面的最低点。

由于线性回归的所有信息都存储在权值向量与偏置值中，所以损失函数实际上可以表示成：

\begin{huge}
\[J = f(w, b)\] 
\end{huge}

或

\begin{huge}
\[J = f(w)\] 
\end{huge}

从 MSE 的表达式可以看出， *对于单个样本来说，* 误差曲面的图像其实也是一个关于权值向量各个分量的凹函数，并且它也连续可导、导数形式简单，因此权值和偏置值每一次训练的时候按照负梯度方向来更新可以使总误差沿着减小最快的方向减小，直到达到极小值。

** 权值根据梯度更新(delta 法则)

既然损失函数可以用这样的方式来求极小值，只要按照梯度的反方向来更新权值，就能得到一个损失函数最小、也就是误差最小的线性回归模型。

也就是说，在使用梯度下降方法训练一个线性回归模型时，每次训练时权值的更新按照以下公式进行：

\begin{huge}
\[
w_i(new) = w_i(old) - \alpha\frac{\partial}{\partial{w_i}}J_w
\] 
\end{huge}

\begin{huge}
\[
\alpha : leaning-rate
\] 
\end{huge}

** 单样本梯度下降(自适应)

尽管线性回归模型追求的是所有样本的总误差最小，但我们依然可以使用和感知机一样的自适应方法(每输入一个样本就更新一次权值)来更新权值，严格来说这并不是梯度下降。用单个样本的输出与真实值产生的误差 E 来对权值向量 w 的某一个分量求偏导，可以得到：

\begin{huge}
\[
\begin{split}
\frac{\partial}{\partial{w_i}}E_w &= \frac{\partial}{\partial{w_i}} \frac{1}{2} (y-t)^2 \\
&= (y-t)x_i \\
\end{split}
\] 
\end{huge}

将这个结果带入权值更新公式，可以得到：

\begin{huge}
\[
w_i(new) = w_i(old) - \alpha(y-t)x_i
\] 
\end{huge}

** 批量梯度下降(batch gradient descent)

正经的梯度下降方法需要考虑到所有样本的总误差，那么总误差对每个权值求偏导就能得到我们需要的梯度，如果损失函数是平方误差，对权值向量的某个分量的偏导结果为：

\begin{huge}
\[
\begin{split}
\frac{\partial}{\partial{w_i}}J_w &= \frac{\partial}{\partial{w_i}} \frac{1}{2N} \sum_{j=0}^N (y_j-t_j)^2 \\
&= \frac{1}{2} \cdot \frac{\partial}{\partial w_i} [ (y_1-t_1)^2 + (y_2-t_2)^2 + ... + (y_N-t_N)^2] \\
&= \frac{1}{2} \sum_{j=0}^N [ 2 (y_j-t_j) x^{(j)}_i ] \\
&= \sum_{j=0}^N [ (y^{(j)}-t^{(j)}) x^{(j)}_i ] \\
\end{split}
\] 
\end{huge}

所以权值的更新公式可以写成：

\begin{huge}
\[
w_i(new) = w_i(old) - \alpha \sum_{j=0}^N [ (y^{(j)}-t^{(j)}) x^{(j)}_i ]
\] 
\end{huge}

* 梯度下降求解逻辑回归(logistic regression)
** 逻辑回归的用途

和感知机一样，逻辑回归要解决的也是用 0-1 来对目标类别进行编码的二分类问题，但是在感知机的基础上使用了更细腻的激活函数，这使得逻辑回归可以输出某个样本的分类概率：

\begin{huge}
\[

P(Y | X = x)

\] 
\end{huge}

** 分类问题转换为回归问题

为什么线性回归不适合用来解决二分类问题：

#+caption: 网上找的图
[[./difference.jpg]]

逻辑回归解决的是一个二分类问题，但使用的方法是把不可度量的目标变量看作可度量，并对其进行回归。

线性回归以及 OLS 方法也可以强行用来做这样的二分类，但会陷入模型输出超过 [0, 1] 范围的困境。

** 为什么使用 sigmoid 函数
*** Odds 发生比

如果以 0-1 来表示二分类问题的类别，并将它看做可度量的值来做线性回归的话，模型的形式会是下面这样：

\begin{huge}
\[
w^Tx = p
\] 
\end{huge}

可是直接用这种方法去解决，就会遇到模型输出的范围的问题。在不脱离线性模型形式的前提下，可以考虑限制左边的值域或拓宽右边的值域，显然后者更为方便，并且可以不引入新的参数就做到这件事情：

\begin{huge}
\[
w^Tx = \frac{p}{1 - p}
\] 
\end{huge}

来看一下当 p 的范围是 [0, 1] 时，$$\frac{p}{1 - p}$$ 的取值情况：

#+begin_src ipython :ipyfile ./beinglogit.png :results raw drawer
# %matplotlib inline is a necessary setting
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0.01, 0.99, 1000)
y = x / (1 - x)
plt.grid()
plt.plot(x, y)
plt.show()
#+end_src

#+RESULTS:
:results:
# Out[21]:
[[file:./beinglogit.png]]
:end:

这样的取值量纲严重地不利于回归，对它取对数试试：

#+begin_src ipython :ipyfile ./logit.png :results raw drawer
# %matplotlib inline is a necessary setting
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0.01, 0.99, 1000)
y = np.log(x / (1 - x))
plt.grid()
plt.plot(x, y)
plt.show()
#+end_src

#+RESULTS:
:results:
# Out[20]:
[[file:./logit.png]]
:end:

上面这个函数被称作 logit 函数，我们现在可以直接用它来作为线性模型的目标：

\begin{huge}
\[
w^Tx = ln(\frac{p}{1-p}) = logit(p)
\] 
\end{huge}

处理掉 ln() :

\begin{huge}
\[
\frac{p}{1-p} = e^{w^Tx}
\] 
\end{huge}

把上式看做关于 p 的方程，解出 p :

\begin{huge}
\[
p = \frac{1}{1 + e^{-w^Tx}} = sigmoid(w^Tx)
\] 
\end{huge}

这就是 sigmoid 函数

*** [超纲] 人口增长微分方程中的 sigmoid 函数
*** [严重超纲] 最大熵模型的二分类形式
** sigmoid 的性质

从机器学习的角度来看，逻辑回归的主体结构与感知机和线性回归差不多，损失函数的选择也可以与线性回归相同，不同之处在于，逻辑回归的激活函数换成了 sigmoid 这个函数：

\begin{huge}
\[
f(x) = \frac{1}{1+e^{-x}}
\] 
\end{huge}

皮埃尔·弗朗索瓦·韦吕勒在 1845 年在研究它与人口增长的关系时曾将它命名为逻辑函数，在机器学习领域一般称它为 sigmoid 函数(意为：S 型的)。

#+begin_src ipython :ipyfile ./sigmoid.png :results raw drawer
# %matplotlib inline is a necessary setting
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np


def sigmoid(x, derivative=False):
    sigm = 1. / (1. + np.exp(-x))
    if derivative:
        return sigm * (1. - sigm)
    return sigm


x = np.linspace(-10, 10, 1000)
y = sigmoid(x)
plt.grid()
plt.plot(x, y)
plt.show()
#+end_src

#+RESULTS:
:results:
# Out[5]:
[[file:./sigmoid.png]]
:end:

由于使用了这个激活函数，模型的输出范围被限制在 (0, 1)，这样在解决 0-1 编码的二分类问题的时候，不会再像直接使用线性回归那样遇到输出过大的问题。

** 损失函数

线性回归模型中，一般用这种形式的损失函数来衡量总误差：

\begin{huge}
\[
J_w = \frac{1}{2n}\sum_{i=1}^{n}(y_i - t_i)^2
\]
\end{huge}

线性回归之所以使用这种损失函数，是因为它形式简单，求导方便，并且是一个关于权值的凹函数。但可惜的是，这个形式简单的损失函数不能用在逻辑回归上，因为当把它看作关于权值的函数时，它并不是一个凹函数(non-convex)。 [fn:1]

所以逻辑回归的损失函数需要用到别的函数作为损失函数。逻辑回归用到的损失函数叫做对数似然损失函数，它可以通过最大似然估计得到。在求解的模型的过程中，为了计算出所有的参数值(权值)，可以使用最大似然估计来得到。

首先，模型的输出为以下形式，代表的是样本对于 1 这个类别的隶属度，也就是 y = 1 的概率：

\begin{huge}
\[
y = sigmoid(u), u = w^Tx
\]
\end{huge}

由于 y 可以视为 0-1 类别中 "1" 这个类别的后验估计，所以可以用以下表达式来表示 "当给定 w 时，x 点属于类别 1 的概率" 。

\begin{huge}
\[
p(t = 1 | x; w) = sigmoid(x)
\]
\end{huge}

那么 x 点属于类别 0 的概率就是：

\begin{huge}
\[
p(t = 0 | x; w) = 1 - sigmoid(x)
\]
\end{huge}

如果某一个表达式在某个参数等于 0 和等于 1 时有两种不同的形式，那么我们可以轻易地把这种形式写在一个统一的式子里面：

\begin{huge}
\[
p(t | x; w) = sigmoid(x)^t \cdot (1 - sigmoid(x))^{1 - t}
\]
\end{huge}

接下来，尝试用最大似然估计来估计出 w:

\begin{huge}
\[
L(w) = P(t|w) = \prod^{n}_{i=1} P(t^{(i)}|x^{(i)};w) = \prod^{n}_{i=1} y^{(i)^{t^{(i)}}} \codt (1 - y^{(i)})^{1 - t^{(i)}}
\]
\end{huge}

对上式的两边取对数：

\begin{huge}
\[
l(w) = lnL(w) = \sum_{i=1}^n t^{(i)}ln(y^{(i)}) + (1 - t{(i)})ln(1 - y{(i)})
\]
\end{huge}

在用最大似然估计来估计 w 的过程中，我们是想使 l(w) 达到最大、并且取此时的 w 作为最终估计值，如果将 l(w) 加上负号，那么训练模型的目的就变成了使 -l(w) 最小化，因次这个 -l(w) 可以作为损失函数。 ~实际上，很多 *简单* 模型的损失函数都是通过在最大似然估计表达式前面加负号或者先取对数再加负号的方式来得到的。~ 综上，逻辑回归的损失函数为：

\begin{huge}
\[
J_w = - \sum_{i=1}^n t^{(i)}ln(y^{(i)}) + (1 - t^{(i)})ln(1 - y^{(i)})
\]
\end{huge}

** 权值更新

在使用梯度下降来训练一个逻辑回归模型时，权值的更新依然按照下面的原则进行：

\begin{huge}
\[
w_i(new) = w_i(old) - \alpha\frac{\partial}{\partial{w_i}}J_w
\] 
\end{huge}

\begin{huge}
\[
\alpha : leaning-rate
\] 
\end{huge}

虽然损失函数已经变化了，但是损失函数对权值的某一分量求偏导的结果形式依然与线性回归相同。

单个样本的误差：

\begin{huge}
\[
\begin{split}
\frac{\partial}{\partial{w_i}}E_w &= \frac{\partial}{\partial{w_i}}[tln(y) + (1 - t)ln(1 - y)] \\
&= (y-t)x_i \\
\end{split}
\] 
\end{huge}

总误差：

\begin{huge}
\[
\begin{split}
\frac{\partial}{\partial{w_i}}J_w &= - \frac{\partial}{\partial{w_i}} \sum_{j=0}^N  [ t^{(j)}ln(y^{(j)}) + (1 - t^{(j)})ln(1 - y^{(j)}) ] \\
&= \sum_{j=0}^N [ (y^{(j)}-t^{(j)}) x^{(j)}_i ] \\
\end{split}
\] 
\end{huge}

* 极大似然估计
极大似然估计，在我们的这个场景里，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。

** 重要假设
极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。

* Footnotes
[fn:1] 想从理论推导来看出它的 non-convex 特性比较难
